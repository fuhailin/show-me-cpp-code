/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

//
// C++ implementation of Magenta melody basic_rnn module
// Author: Rock Zhuang
// Date  : Dec 27, 2018
//

#include <cassert>

#include "tensorflow/cc/client/client_session.h"
#include "tensorflow/cc/ops/dataset_ops_internal.h"
#include "tensorflow/cc/ops/rnn_ops_internal.h"
#include "tensorflow/cc/ops/standard_ops.h"
#include "tensorflow/cc/training/queue_runner.h"
#include "tensorflow/core/protobuf/queue_runner.pb.h"

using namespace tensorflow;
using namespace tensorflow::ops;
using namespace tensorflow::ops::internal;
using namespace std;

// #define VERBOSE 1
// #define TESTING 1

#define HIDDEN_SIZE 128
#define SEQ_LENGTH 64  // batch_size

// (DEFAULT_MAX_NOTE(84) - DEFAULT_MIN_NOTE(48) + NUM_SPECIAL_MELODY_EVENTS(2))
#define VOCAB_SIZE 38

#define TEST_SEQ_LENGTH 1
#define TRAINING_STEPS 10000

namespace tensorflow {

class InternalClientSession {
 public:
  static tensorflow::Session* GetSession(tensorflow::ClientSession& session) {
    return session.GetSession();
  }
};

}  // namespace tensorflow

QueueRunnerDef BuildQueueRunnerDef(
    const std::string& queue_name, const std::vector<std::string>& enqueue_ops,
    const std::string& close_op, const std::string& cancel_op,
    const std::vector<tensorflow::error::Code>& queue_closed_error_codes) {
  QueueRunnerDef queue_runner_def;
  *queue_runner_def.mutable_queue_name() = queue_name;
  for (const std::string& enqueue_op : enqueue_ops) {
    *queue_runner_def.mutable_enqueue_op_name()->Add() = enqueue_op;
  }
  *queue_runner_def.mutable_close_op_name() = close_op;
  *queue_runner_def.mutable_cancel_op_name() = cancel_op;
  for (const auto& error_code : queue_closed_error_codes) {
    *queue_runner_def.mutable_queue_closed_exception_types()->Add() =
        error_code;
  }
  return queue_runner_def;
}

int main() {
  Scope root = Scope::NewRootScope();

  //
  // Dataset parsing
  //

  //
  // The file 'training_melodies.tfrecord' was generated by running the
  // following commands from Magenta project: (put midi files into the folder
  // '/tmp/data/midi/' in advance)
  //   $ convert_dir_to_note_sequences --input_dir=/tmp/data/midi/ \
  //       --output_file=/tmp/data/notesequences.tfrecord --recursive
  //   $ melody_rnn_create_dataset --config=basic_rnn \
  //   --input=/tmp/data/notesequences.tfrecord --output_dir=/tmp/data/ \
  //   --eval_ratio=0.0
  //

  // TFRecordDataset
  Tensor filenames("/tmp/data/training_melodies.tfrecord");
  Tensor compression_type("");
  Tensor buffer_size((int64)1024);

  Output tfrecord_dataset =
      TFRecordDataset(root, filenames, compression_type, buffer_size);
  auto shuffle_and_repeat_dataset = ShuffleAndRepeatDataset(
      root, tfrecord_dataset, Cast(root, 5, DT_INT64),   // buffer_size
      Cast(root, 0, DT_INT64), Cast(root, 0, DT_INT64),  // seedX
      Cast(root, 10, DT_INT64),  // count, -1 for infinite repetition
      std::initializer_list<DataType>{DT_STRING},
      std::initializer_list<PartialTensorShape>{{}});
  Output iterator_output =
      Iterator(root, "iterator1", "", vector<DataType>({DT_STRING}),
               vector<PartialTensorShape>({{}}));
  Operation make_iterator_op =
      MakeIterator(root, shuffle_and_repeat_dataset, iterator_output);
  auto iterator_get_next =
      IteratorGetNext(root, iterator_output, vector<DataType>({DT_STRING}),
                      vector<PartialTensorShape>({{}}));

  // Input for ParseExample
  Tensor feature_list_dense_missing_assumed_empty(DT_STRING, TensorShape({0}));

  vector<Output> feature_list_sparse_keys, feature_list_sparse_types,
      feature_list_dense_keys, feature_list_dense_defaults;

  DataTypeSlice feature_list_dense_types = {DT_INT64, DT_FLOAT};
  gtl::ArraySlice<PartialTensorShape> feature_list_dense_shapes = {
      {}, {VOCAB_SIZE}};

  vector<Output> context_sparse_keys, context_sparse_types, context_dense_keys,
      context_dense_types, context_dense_defaults, context_dense_shapes;

  feature_list_dense_keys.push_back(
      Const<string>(root, "labels", TensorShape({})));
  feature_list_dense_keys.push_back(
      Const<string>(root, "inputs", TensorShape({})));

  feature_list_dense_defaults.push_back(Const<int64>(root, 2, TensorShape({})));
  feature_list_dense_defaults.push_back(
      Const<float>(root, 1, TensorShape({VOCAB_SIZE})));

  // ParseSingleSequenceExample parse only one sequence. ParseSequenceExample
  // supports batch inputs
  auto parse_single_sequence_example = ParseSingleSequenceExample(
      root, iterator_get_next[0], feature_list_dense_missing_assumed_empty,
      InputList(context_sparse_keys), InputList(context_dense_keys),
      InputList(feature_list_sparse_keys), InputList(feature_list_dense_keys),
      InputList(context_dense_defaults),
      Const<string>(root, "melody_rnn_training sequence parsing",
                    TensorShape({})),
      ParseSingleSequenceExample::Attrs()
          .FeatureListDenseTypes(feature_list_dense_types)
          .FeatureListDenseShapes(feature_list_dense_shapes));

  // QueueRunner
  constexpr char kCancelOp[] = "cancel0";
  constexpr char kCloseOp[] = "close0";
  constexpr char kDequeueOp[] = "dequeue0";
  constexpr char kDequeueOp1[] = "dequeue0:1";
  constexpr char kEnqueueOp[] = "enqueue0";
  constexpr char kQueueName[] = "fifoqueue";

  auto pfq = FIFOQueue(root.WithOpName(kQueueName),
                       {DataType::DT_INT64, DataType::DT_FLOAT});
  auto enqueue = QueueEnqueue(
      root.WithOpName(kEnqueueOp), pfq,
      InputList(parse_single_sequence_example.feature_list_dense_values));
  auto closequeue = QueueClose(root.WithOpName(kCloseOp), pfq);
  auto cancelqueue = QueueClose(root.WithOpName(kCancelOp), pfq,
                                QueueClose::CancelPendingEnqueues(true));
  // QueueDequeueMany to deque multiple items as a batch
  auto dequeue = QueueDequeue(root.WithOpName(kDequeueOp), pfq,
                              {DataType::DT_INT64, DataType::DT_FLOAT});

  // Session
  // Note that ClientSession can extend graph before running, Session cannot.
  vector<Tensor> dataset_outputs;
  ClientSession session(root);

  // Run make_iterator_output first
  TF_CHECK_OK(session.Run({}, {}, {make_iterator_op}, nullptr));

  // Coordinator and QueueRunner
  QueueRunnerDef queue_runner =
      BuildQueueRunnerDef(kQueueName, {kEnqueueOp}, kCloseOp, kCancelOp,
                          {tensorflow::error::Code::OUT_OF_RANGE,
                           tensorflow::error::Code::CANCELLED});
  Coordinator coord;
  std::unique_ptr<QueueRunner> qr;
  TF_CHECK_OK(QueueRunner::New(queue_runner, &coord, &qr));
  TF_CHECK_OK(qr->Start(InternalClientSession::GetSession(session)));
  TF_CHECK_OK(coord.RegisterRunner(std::move(qr)));

  while (session
             .Run(RunOptions(), {}, {kDequeueOp, kDequeueOp1}, {},
                  &dataset_outputs)
             .ok()) {
#ifdef VERBOSE
    LOG(INFO) << "Print deque: " << dataset_outputs[0].DebugString() << ", "
              << dataset_outputs[1].DebugString();
    // for(int i = 0; i < dataset_outputs[0].NumElements(); i++) {
    //   LOG(INFO) << "Print labels: " << dataset_outputs[0].vec<int64>()(i);
    // }
#endif
  }
  // For now, only the lastest sequence example is used below

  //
  // Train and Eval
  //

  // Trainable parameters start here, to be improved
  auto w_xh = Variable(root, {HIDDEN_SIZE, VOCAB_SIZE}, DT_FLOAT);
  auto rate = Const(root, {0.01f});
  auto random_value = RandomNormal(root, {HIDDEN_SIZE, VOCAB_SIZE}, DT_FLOAT);
  auto assign_w_xh = Assign(root, w_xh, Multiply(root, random_value, rate));

  auto w_hh = Variable(root, {HIDDEN_SIZE, HIDDEN_SIZE}, DT_FLOAT);
  auto random_value2 = RandomNormal(root, {HIDDEN_SIZE, HIDDEN_SIZE}, DT_FLOAT);
  auto assign_w_hh = Assign(root, w_hh, Multiply(root, random_value2, rate));

  auto w_hy = Variable(root, {VOCAB_SIZE, HIDDEN_SIZE}, DT_FLOAT);
  auto random_value3 = RandomNormal(root, {VOCAB_SIZE, HIDDEN_SIZE}, DT_FLOAT);
  auto assign_w_hy = Assign(root, w_hy, Multiply(root, random_value3, rate));

  auto b_h = Variable(root, {HIDDEN_SIZE, 1}, DT_FLOAT);
  Tensor b_h_zero_tensor(DT_FLOAT, TensorShape({HIDDEN_SIZE, 1}));
  b_h_zero_tensor.matrix<float>().setZero();
  auto assign_b_h = Assign(root, b_h, ZerosLike(root, b_h_zero_tensor));

  auto b_y = Variable(root, {VOCAB_SIZE, 1}, DT_FLOAT);
  Tensor b_y_zero_tensor(DT_FLOAT, TensorShape({VOCAB_SIZE, 1}));
  b_y_zero_tensor.matrix<float>().setZero();
  auto assign_b_y = Assign(root, b_y, ZerosLike(root, b_y_zero_tensor));
  // Trainable parameters end here

  // Gradient accum parameters start here
  auto ada_w_xh = Variable(root, {HIDDEN_SIZE, VOCAB_SIZE}, DT_FLOAT);
  auto assign_ada_w_xh = Assign(root, ada_w_xh, ZerosLike(root, w_xh));

  auto ada_w_hh = Variable(root, {HIDDEN_SIZE, HIDDEN_SIZE}, DT_FLOAT);
  auto assign_ada_w_hh = Assign(root, ada_w_hh, ZerosLike(root, w_hh));

  auto ada_w_hy = Variable(root, {VOCAB_SIZE, HIDDEN_SIZE}, DT_FLOAT);
  auto assign_ada_w_hy = Assign(root, ada_w_hy, ZerosLike(root, w_hy));

  auto ada_b_h = Variable(root, {HIDDEN_SIZE, 1}, DT_FLOAT);
  auto assign_ada_b_h = Assign(root, ada_b_h, ZerosLike(root, b_h));

  auto ada_b_y = Variable(root, {VOCAB_SIZE, 1}, DT_FLOAT);
  auto assign_ada_b_y = Assign(root, ada_b_y, ZerosLike(root, b_y));
  // Gradient accum parameters end here

  // Placeholders
  auto x = Placeholder(root, DT_FLOAT,
                       Placeholder::Shape({SEQ_LENGTH, VOCAB_SIZE, 1}));
  auto y = Placeholder(root, DT_FLOAT, Placeholder::Shape({SEQ_LENGTH}));
  auto h_prev =
      Placeholder(root, DT_FLOAT, Placeholder::Shape({HIDDEN_SIZE, 1}));

  // VanillaRNN
  auto vanilla_rnn = VanillaRNN(root, x, y, h_prev, w_xh, w_hh, w_hy, b_h, b_y,
                                VanillaRNN::Attrs().Hidsize(HIDDEN_SIZE));

  // VanillaRNN and TopK For Eval begins
  // Placeholders
  auto x_eval = Placeholder(
      root, DT_FLOAT, Placeholder::Shape({TEST_SEQ_LENGTH, VOCAB_SIZE, 1}));
  auto y_eval =
      Placeholder(root, DT_FLOAT, Placeholder::Shape({TEST_SEQ_LENGTH}));

  // VanillaRNN
  auto vanilla_rnn_eval =
      VanillaRNN(root, x_eval, y_eval, h_prev, w_xh, w_hh, w_hy, b_h, b_y,
                 VanillaRNN::Attrs().Hidsize(HIDDEN_SIZE));

  // Top 1
  auto topk_input =
      Placeholder(root, DT_FLOAT, Placeholder::Shape({VOCAB_SIZE}));
  auto topk = TopK(root, topk_input, Cast(root, 1, DT_INT32));
  // VanillaRNN and TopK For Eval ends

  // VanillaRNNGrad
  auto vanilla_rnn_grad =
      VanillaRNNGrad(root, x, y, vanilla_rnn.p, vanilla_rnn.h, w_hh, w_hy,
                     h_prev, VanillaRNNGrad::Attrs().Hidsize(HIDDEN_SIZE));

  // Gradient
  auto lr = Cast(root, 0.06, DT_FLOAT);

  // alternative of ApplyAdagrad
  Output apply_w_xh =
      ApplyAdagradTrick(root, w_xh, ada_w_xh, lr, vanilla_rnn_grad.d_w_xh);
  Output apply_w_hh =
      ApplyAdagradTrick(root, w_hh, ada_w_hh, lr, vanilla_rnn_grad.d_w_hh);
  Output apply_w_hy =
      ApplyAdagradTrick(root, w_hy, ada_w_hy, lr, vanilla_rnn_grad.d_w_hy);
  Output apply_b_h =
      ApplyAdagradTrick(root, b_h, ada_b_h, lr, vanilla_rnn_grad.d_b_h);
  Output apply_b_y =
      ApplyAdagradTrick(root, b_y, ada_b_y, lr, vanilla_rnn_grad.d_b_y);

  // Initialize variables
  TF_CHECK_OK(session.Run(
      {assign_w_xh, assign_w_hh, assign_w_hy, assign_b_h, assign_b_y},
      nullptr));
  TF_CHECK_OK(session.Run({assign_ada_w_xh, assign_ada_w_hh, assign_ada_w_hy,
                           assign_ada_b_h, assign_ada_b_y},
                          nullptr));

  // Train and eval
  int step = 0;
  while (step < TRAINING_STEPS) {
    // content index
    int content_index = 0;

    // Prepare hidden tensor
    Tensor h_prev_tensor(DT_FLOAT, TensorShape({HIDDEN_SIZE, 1}));
    typename TTypes<float>::Matrix h_prev_t = h_prev_tensor.matrix<float>();
    h_prev_t.setZero();
#ifdef VERBOSE
    LOG(INFO) << __FUNCTION__ << "----------------h_prev_t: " << endl
              << h_prev_t;
#endif

    int seq_batches = dataset_outputs[0].NumElements() / SEQ_LENGTH;
    // Loop in test content
    for (int bidx = 0; bidx < seq_batches; bidx++) {
      // Evaluate
      if (step % 1000 == 0) {
        // Batch input with batch size of TEST_SEQ_LENGTH
        Tensor x_tensor(DT_FLOAT,
                        TensorShape({TEST_SEQ_LENGTH, VOCAB_SIZE, 1}));
        auto e_2d =
            x_tensor.shaped<float, 2>({TEST_SEQ_LENGTH, VOCAB_SIZE * 1});

        Eigen::DSizes<Eigen::DenseIndex, 2> indices_dataset(content_index, 0);
        Eigen::DSizes<Eigen::DenseIndex, 2> sizes_dataset(1, VOCAB_SIZE * 1);
        Eigen::Tensor<float, 2, Eigen::RowMajor> mat =
            dataset_outputs[1].matrix<float>().slice(indices_dataset,
                                                     sizes_dataset);

#ifdef VERBOSE
        LOG(INFO) << __FUNCTION__ << "----------------Evaluate mat: " << mat;
#endif

        // Prepare y
        Tensor y_tensor(DT_FLOAT, TensorShape({TEST_SEQ_LENGTH}));
        y_tensor.vec<float>()(0) =
            0;  // y value make no sense in the evaluation process

        Tensor eval_h_prev_tensor(DT_FLOAT, TensorShape({HIDDEN_SIZE, 1}));
        eval_h_prev_tensor = h_prev_tensor;

        for (int i = 0; i < 20; i++) {
          // set e_2d
          Eigen::DSizes<Eigen::DenseIndex, 2> indices(0, 0);
          Eigen::DSizes<Eigen::DenseIndex, 2> sizes(1, VOCAB_SIZE * 1);
          e_2d.slice(indices, sizes) = mat;

          vector<Tensor> outputs;
          vector<Tensor> outputs_topk;

          // Run
          TF_CHECK_OK(session.Run({{x_eval, x_tensor},
                                   {y_eval, y_tensor},
                                   {h_prev, eval_h_prev_tensor}},
                                  {vanilla_rnn_eval.p, vanilla_rnn_eval.h}, {},
                                  &outputs));
#ifdef VERBOSE
          LOG(INFO) << "Print vanilla_rnn_eval, step: " << step
                    << ", debug: " << outputs[0].DebugString() << ", "
                    << outputs[1].DebugString();
#endif

          // top 1
          Tensor topk_input_tensor(DT_FLOAT, TensorShape({VOCAB_SIZE}));
          topk_input_tensor.flat<float>() = outputs[0].flat<float>();
          TF_CHECK_OK(session.Run({{topk_input, topk_input_tensor}},
                                  {topk.values, topk.indices}, {},
                                  &outputs_topk));
#ifdef VERBOSE
          LOG(INFO) << "Print topk, step: " << step
                    << ", debug: " << outputs_topk[0].DebugString() << ", "
                    << outputs_topk[1].DebugString();
#endif

          // update mat for x_tensor
          int index = outputs_topk[1].scalar<int>()();
          mat.setZero();
          mat(0, index) = 1;

          // #ifdef VERBOSE
          LOG(INFO) << __FUNCTION__
                    << "----------------Evaluate index: " << index;
          // LOG(INFO) << __FUNCTION__ << "----------------Evaluate mat: " <<
          // mat;
          // #endif

          // update eval_h_prev_tensor
          CHECK(eval_h_prev_tensor.CopyFrom(
              outputs[1].Slice(0, 1),
              {outputs[1].dim_size(1), outputs[1].dim_size(2)}));
        }  // for(int i = 0; i < 20; i++) {
      }    // Evaluate

      // Train
      {
        // Batch input with batch size of SEQ_LENGTH
        Tensor x_tensor(DT_FLOAT, TensorShape({SEQ_LENGTH, VOCAB_SIZE, 1}));

        // batch
        auto e_2d = x_tensor.shaped<float, 2>({SEQ_LENGTH, VOCAB_SIZE * 1});
        int x_index = content_index;
        for (int i = 0; i < SEQ_LENGTH; i++) {
          Eigen::DSizes<Eigen::DenseIndex, 2> indices_dataset(x_index++, 0);
          Eigen::DSizes<Eigen::DenseIndex, 2> sizes_dataset(1, VOCAB_SIZE * 1);
          Eigen::Tensor<float, 2, Eigen::RowMajor> mat =
              dataset_outputs[1].matrix<float>().slice(indices_dataset,
                                                       sizes_dataset);

          // set e_2d
          Eigen::DSizes<Eigen::DenseIndex, 2> indices(i, 0);
          Eigen::DSizes<Eigen::DenseIndex, 2> sizes(1, VOCAB_SIZE * 1);
          e_2d.slice(indices, sizes) = mat;
        }

#ifdef VERBOSE
        // Check e_2d for funfor i in range(len(data)/seq_length):
        LOG(INFO) << __FUNCTION__ << "----------------e_2d: " << endl << e_2d;

        // Check x_tensor for fun
        auto e_t = x_tensor.tensor<float, 3>();
        LOG(INFO) << __FUNCTION__ << "----------------e_t: " << endl << e_t;
#endif

        // Prepare y
        Tensor y_tensor(DT_FLOAT, TensorShape({SEQ_LENGTH}));
        typename TTypes<float>::Vec y_t = y_tensor.vec<float>();

        // Prepare y
        int y_index = content_index;
        for (int i = 0; i < SEQ_LENGTH; i++) {
          y_t(i) =
              static_cast<float>(dataset_outputs[0].vec<int64>()(y_index++));
        }

        //
        content_index += SEQ_LENGTH;

#ifdef VERBOSE
        LOG(INFO) << __FUNCTION__ << "----------------y_t: " << endl << y_t;
#endif

        vector<Tensor> outputs;

        // Run
        TF_CHECK_OK(
            session.Run({{x, x_tensor}, {y, y_tensor}, {h_prev, h_prev_tensor}},
                        {vanilla_rnn.h, vanilla_rnn.loss, apply_w_xh,
                         apply_w_hh, apply_w_hy, apply_b_h, apply_b_y},
                        {}, &outputs));

        if (step % 1000 == 0)
          LOG(INFO) << "Print step: " << step
                    << ", loss: " << outputs[1].DebugString();

        // Update h_prev
        CHECK(h_prev_tensor.CopyFrom(
            outputs[0].Slice(SEQ_LENGTH - 1, SEQ_LENGTH),
            {outputs[0].dim_size(1), outputs[0].dim_size(2)}));
#ifdef VERBOSE
        LOG(INFO) << __FUNCTION__
                  << "----------------------------h_prev_tensor updated:"
                  << endl
                  << h_prev_tensor.matrix<float>();
#endif
      }  // Train

      step++;
    }  // for(int bidx = 0; bidx < seq_batches; bidx++) {
  }    // while(step < TRAINING_STEPS) {

  // Stop
  TF_CHECK_OK(coord.RequestStop());
  TF_CHECK_OK(coord.Join());

  return 0;
}
