/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "myexamples/test_tf/gan/dcgan/tf_ops.h"

#include <algorithm>
#include <vector>

#define MOMENTUM 0.99f

namespace tensorflow {
namespace ops {

TFVariable::TFVariable(const ::tensorflow::Scope& scope,
                       PartialTensorShape shape, DataType dtype,
                       const Variable::Attrs& attrs, bool trainable) {
    this->output = Variable(scope, shape, dtype, attrs);

    if (trainable) scope.AddTrainableVariable(this->output, shape);
}

TFVariable::TFVariable(const ::tensorflow::Scope& scope,
                       PartialTensorShape shape, DataType dtype, bool trainable)
    : TFVariable(scope, shape, dtype, Variable::Attrs(), trainable) {}

TFAssign::TFAssign(const ::tensorflow::Scope& scope, ::tensorflow::Input ref,
                   ::tensorflow::Input value, const Assign::Attrs& attrs) {
    this->output = Assign(scope, ref, value, attrs);

    scope.AddAssignOp(this->output);
}

TFAssign::TFAssign(const ::tensorflow::Scope& scope, ::tensorflow::Input ref,
                   ::tensorflow::Input value)
    : TFAssign(scope, ref, value, Assign::Attrs()) {}

Moments::Moments(const ::tensorflow::Scope& scope, const ::tensorflow::Input& x,
                 const std::initializer_list<int>& axes, bool keep_dims) {
    auto m = ReduceMean(scope, x, Input(axes), ReduceMean::KeepDims(true));

    auto sg = StopGradient(scope, m);
    auto sd = SquaredDifference(scope, x, sg);
    auto v = ReduceMean(scope, sd, Input(axes), ReduceMean::KeepDims(true));

    if (keep_dims) {
        this->mean = m;
        this->variance = v;
    } else {
        this->mean = Squeeze(scope, m, Squeeze::Axis(axes));
        this->variance = Squeeze(scope, v, Squeeze::Axis(axes));
    }
}

// tf.nn.batch_normalization
// def batch_normalization(x,
//                         mean,
//                         variance,
//                         offset,
//                         scale,
//                         variance_epsilon,
//                         name=None):
//   with ops.name_scope(name, "batchnorm", [x, mean, variance, scale, offset]):
//     inv = math_ops.rsqrt(variance + variance_epsilon)
//     if scale is not None:
//       inv *= scale
//     # Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on
//     # the precise order of ops that are generated by the expression below.
//     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(
//         offset - mean * inv if offset is not None else -mean * inv, x.dtype)
BatchNormalization::BatchNormalization(
    const ::tensorflow::Scope& scope, const ::tensorflow::Input& x,
    const ::tensorflow::Input& mean, const ::tensorflow::Input& variance,
    const ::tensorflow::Input& offset, const ::tensorflow::Input& scale,
    const ::tensorflow::Input& variance_epsilon) {
    auto inv = Multiply(
        scope, Rsqrt(scope, Add(scope, variance, variance_epsilon)), scale);
    LOG(INFO) << "Node building status: " << scope.status();

    auto tmp1 = Multiply(scope, x, Cast(scope, inv, DT_FLOAT));
    LOG(INFO) << "Node building status: " << scope.status();

    auto tmp2 = Multiply(scope, mean, inv);
    LOG(INFO) << "Node building status: " << scope.status();

    this->output =
        Add(scope, tmp1, Cast(scope, Sub(scope, offset, tmp2), DT_FLOAT));
}

Dropout::Dropout(const ::tensorflow::Scope& scope, const ::tensorflow::Input x,
                 const int rate) {
    float keep_prob = 1 - rate;
    auto random_value5 = RandomUniform(scope, Shape(scope, x), DT_FLOAT);
    LOG(INFO) << "Node building status: " << scope.status();

    auto random_tensor =
        Add(scope, random_value5, Const<float>(scope, {keep_prob}));
    LOG(INFO) << "Node building status: " << scope.status();

    auto binary_tensor = Floor(scope, random_tensor);
    LOG(INFO) << "Node building status: " << scope.status();

    this->output = Multiply(
        scope, Div(scope, x, Const<float>(scope, {keep_prob})), binary_tensor);
}

// python code:
//     # The logistic loss formula from above is
//     #   x - x * z + log(1 + exp(-x))
//     # For x < 0, a more numerically stable formula is
//     #   -x * z + log(1 + exp(x))
//     # Note that these two expressions can be combined into the following:
//     #   max(x, 0) - x * z + log(1 + exp(-abs(x)))
//     # To allow computing gradients at zero, we define custom versions of max
//     and # abs functions. zeros = array_ops.zeros_like(logits,
//     dtype=logits.dtype) cond = (logits >= zeros) relu_logits =
//     array_ops.where(cond, logits, zeros) neg_abs_logits =
//     array_ops.where(cond, -logits, logits) return math_ops.add(
//         relu_logits - logits * labels,
//         math_ops.log1p(math_ops.exp(neg_abs_logits)),
//         name=name)
SigmoidCrossEntropyWithLogits::SigmoidCrossEntropyWithLogits(
    const ::tensorflow::Scope& scope, const ::tensorflow::Input labels,
    const ::tensorflow::Input logits) {
    auto zeros = ZerosLike(scope, logits);
    auto cond = GreaterEqual(scope, logits, zeros);
    auto relu_logits = SelectV2(scope, cond, logits, zeros);
    auto neg_abs_logits = SelectV2(scope, cond, Negate(scope, logits), logits);

    this->output =
        Add(scope, Sub(scope, relu_logits, Multiply(scope, logits, labels)),
            Log1p(scope, Exp(scope, neg_abs_logits)));
}

// Only DT_FLOAT and 2D/4D shape is supported for now
GlorotUniform::GlorotUniform(const ::tensorflow::Scope& scope,
                             const std::initializer_list<int64>& shape) {
    // RandomUniform
    auto random_value =
        RandomUniform(scope, Const(scope, Input::Initializer(shape)), DT_FLOAT);
    LOG(INFO) << "Node building status: " << scope.status();

    std::vector<int64> shape_vec(shape);

    // For 2D
    float fan_in = shape_vec[0];
    float fan_out = shape_vec[1];

    // For 4D
    if (shape_vec.size() == 4) {
        float receptive_field_size = 1.0f * shape_vec[0] * shape_vec[1];
        fan_in = receptive_field_size * shape_vec[2];
        fan_out = receptive_field_size * shape_vec[3];
    }

    // Python code:
    //   scale /= max(1., (fan_in + fan_out) / 2.)
    //   limit = math.sqrt(3.0 * scale) => minval is -limit, maxval is limit
    //   result = math_ops.add(rnd * (maxval - minval), minval, name=name)
    float scale = 1.0f / std::max(1.0f, (fan_in + fan_out) / 2.0f);
    float limit = std::sqrt(3.0f * scale);
    float maxval = limit;
    float minval = -limit;
    auto result =
        Add(scope,
            Multiply(scope, random_value, Const<float>(scope, (maxval - minval))),
            Const<float>(scope, minval));
    LOG(INFO) << "Node building status: " << scope.status();

    this->output = result;
}

// Conv2DTranspose
Conv2DTranspose::Conv2DTranspose(const ::tensorflow::Scope& scope,
                                 const ::tensorflow::Input& input_sizes,
                                 const ::tensorflow::Input& filter,
                                 const ::tensorflow::Input& out_backprop,
                                 const gtl::ArraySlice<int>& strides,
                                 const StringPiece padding) {
    // Conv2DBackpropInput
    this->output = Conv2DBackpropInput(scope, input_sizes, filter, out_backprop,
                                       strides, padding);
}

TFBatchNormalization::TFBatchNormalization(const ::tensorflow::Scope& scope,
                                           const PartialTensorShape& shape) {
    // moving mean and variance
    this->moving_mean = Variable(scope, shape, DT_FLOAT);
    TFAssign(scope, this->moving_mean, ZerosLike(scope, this->moving_mean));

    this->moving_variance = Variable(scope, shape, DT_FLOAT);
    TFAssign(scope, this->moving_variance,
             OnesLike(scope, this->moving_variance));

    // gamma
    this->gamma = TFVariable(scope.WithOpName("gamma"), shape, DT_FLOAT, true);
    TFAssign(scope, this->gamma, OnesLike(scope, this->gamma));
    LOG(INFO) << "Node building status: " << scope.status();

    // beta
    this->beta = TFVariable(scope.WithOpName("beta"), shape, DT_FLOAT, true);
    TFAssign(scope, this->beta, ZerosLike(scope, this->beta));
    LOG(INFO) << "Node building status: " << scope.status();
}

Output TFBatchNormalization::Build(const ::tensorflow::Scope& scope,
                                   const ::tensorflow::Input& x,
                                   const std::initializer_list<int>& axes,
                                   const ::tensorflow::Input& variance_epsilon,
                                   bool training) {
    Output mean;
    Output variance;

    if (training) {
        // mean and variance
        auto moments = Moments(scope, x, axes, false);
        mean = moments.mean;
        variance = moments.variance;

        // update ops
        auto decay = Const<float>(scope, 1.0f - MOMENTUM, {});
        auto update_delta1 =
            Multiply(scope, Sub(scope, this->moving_mean, mean), decay);
        auto update_moving_mean = AssignSub(scope.WithOpName("update_moving_mean"),
                                            this->moving_mean, update_delta1);
        scope.AddUpdateOp(update_moving_mean);
        LOG(INFO) << "Node building status: " << scope.status();

        auto update_delta2 =
            Multiply(scope, Sub(scope, this->moving_variance, variance), decay);
        auto update_moving_variance =
            AssignSub(scope.WithOpName("update_moving_variance"),
                      this->moving_variance, update_delta2);
        scope.AddUpdateOp(update_moving_variance);
        LOG(INFO) << "Node building status: " << scope.status();
    } else {
        mean = this->moving_mean;
        variance = this->moving_variance;
    }

#ifdef DEBUG
    mean = Print(scope, mean, {mean},
                 Print::Message("Print------------------mean: "));
    variance = Print(scope, variance, {variance},
                     Print::Message("Print------------------variance: "));
#endif

    // output
    return BatchNormalization(scope, x, mean, variance, this->beta, this->gamma,
                              variance_epsilon);
}

TFFusedBatchNorm::TFFusedBatchNorm(const ::tensorflow::Scope& scope,
                                   const PartialTensorShape& shape) {
    // moving mean and variance
    this->moving_mean = Variable(scope, shape, DT_FLOAT);
    TFAssign(scope, this->moving_mean, ZerosLike(scope, this->moving_mean));

    this->moving_variance = Variable(scope, shape, DT_FLOAT);
    TFAssign(scope, this->moving_variance,
             OnesLike(scope, this->moving_variance));

    // gamma
    this->gamma =
        TFVariable(scope.WithOpName("fused_gamma"), shape, DT_FLOAT, true);
    TFAssign(scope, this->gamma, OnesLike(scope, this->gamma));
    LOG(INFO) << "Node building status: " << scope.status();

    // beta
    this->beta =
        TFVariable(scope.WithOpName("fused_beta"), shape, DT_FLOAT, true);
    TFAssign(scope, this->beta, ZerosLike(scope, this->beta));
    LOG(INFO) << "Node building status: " << scope.status();
}

Output TFFusedBatchNorm::Build(const ::tensorflow::Scope& scope,
                               const ::tensorflow::Input& x,
                               const float variance_epsilon, bool training) {
    if (training) {
        // mean and variance
        auto mean = Const<float>(scope, {});
        auto variance = Const<float>(scope, {});

        auto fused_batch_norm =
            FusedBatchNorm(scope, x, this->gamma, this->beta, mean, variance,
                           FusedBatchNorm::Epsilon(variance_epsilon));

        // update ops
        auto decay = Const<float>(scope, 1.0f - MOMENTUM, {});
        auto update_delta1 = Multiply(
            scope, Sub(scope, this->moving_mean, fused_batch_norm.batch_mean),
            decay);
        auto update_moving_mean =
            AssignSub(scope.WithOpName("fused_update_moving_mean"),
                      this->moving_mean, update_delta1);
        scope.AddUpdateOp(update_moving_mean);
        LOG(INFO) << "Node building status: " << scope.status();

        auto update_delta2 = Multiply(
            scope,
            Sub(scope, this->moving_variance, fused_batch_norm.batch_variance),
            decay);
        auto update_moving_variance =
            AssignSub(scope.WithOpName("fused_update_moving_variance"),
                      this->moving_variance, update_delta2);
        scope.AddUpdateOp(update_moving_variance);
        LOG(INFO) << "Node building status: " << scope.status();

        return fused_batch_norm.y;
    } else {
        auto mean = this->moving_mean;
        auto variance = this->moving_variance;

        auto fused_batch_norm = FusedBatchNorm(
            scope, x, this->gamma, this->beta, mean, variance,
            FusedBatchNorm::Epsilon(variance_epsilon).IsTraining(false));

        return fused_batch_norm.y;
    }
}

}  // namespace ops
}  // namespace tensorflow
